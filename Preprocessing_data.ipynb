{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df=pd.read_csv('C:\\\\Users\\\\user\\\\Twitter analysis Project\\\\data\\\\tweets_sentiment.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Replace Emoticons with text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>date</th>\n",
       "      <th>user</th>\n",
       "      <th>location</th>\n",
       "      <th>keyword</th>\n",
       "      <th>scores</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15601</th>\n",
       "      <td>Appointments for COVID-19 vaccinations are ava...</td>\n",
       "      <td>5/11/2021 16:58</td>\n",
       "      <td>mcculloughmg</td>\n",
       "      <td>Kalamazoo, MI</td>\n",
       "      <td>covid-19</td>\n",
       "      <td>{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15602</th>\n",
       "      <td>*COVID-19 TESTING IN SOUTH TRINIDAD* A drive-i...</td>\n",
       "      <td>5/11/2021 16:58</td>\n",
       "      <td>KyJuve10</td>\n",
       "      <td>Trinidad and Tobago</td>\n",
       "      <td>covid-19</td>\n",
       "      <td>{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15603</th>\n",
       "      <td>Uber, Lyft to Provide Free Rides to Covid-19 V...</td>\n",
       "      <td>5/11/2021 16:58</td>\n",
       "      <td>masonis_marilyn</td>\n",
       "      <td>Michigan, USA</td>\n",
       "      <td>covid-19</td>\n",
       "      <td>{'neg': 0.0, 'neu': 0.784, 'pos': 0.216, 'comp...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15604</th>\n",
       "      <td>In order to maintain public health and safety ...</td>\n",
       "      <td>5/11/2021 16:58</td>\n",
       "      <td>sarcast87934056</td>\n",
       "      <td>Oman</td>\n",
       "      <td>covid-19</td>\n",
       "      <td>{'neg': 0.092, 'neu': 0.792, 'pos': 0.117, 'co...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15605</th>\n",
       "      <td>#Goa's decision to administer #ivermectin to a...</td>\n",
       "      <td>5/11/2021 16:58</td>\n",
       "      <td>r_umeash</td>\n",
       "      <td>Satara, India</td>\n",
       "      <td>covid-19</td>\n",
       "      <td>{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21995</th>\n",
       "      <td>CIOs and CTOs say they adapted well to disrupt...</td>\n",
       "      <td>5/12/2021 19:44</td>\n",
       "      <td>neuzida</td>\n",
       "      <td>Remote</td>\n",
       "      <td>covid-19</td>\n",
       "      <td>{'neg': 0.145, 'neu': 0.727, 'pos': 0.127, 'co...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21996</th>\n",
       "      <td>U.S. health advisors endorsed the use of Pfize...</td>\n",
       "      <td>5/12/2021 19:44</td>\n",
       "      <td>News12BK</td>\n",
       "      <td>Brooklyn, NY</td>\n",
       "      <td>covid-19</td>\n",
       "      <td>{'neg': 0.0, 'neu': 0.909, 'pos': 0.091, 'comp...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21997</th>\n",
       "      <td>Fact-checking the Pandemic: the Truth about CO...</td>\n",
       "      <td>5/12/2021 19:44</td>\n",
       "      <td>real_gtk</td>\n",
       "      <td>NaN</td>\n",
       "      <td>covid-19</td>\n",
       "      <td>{'neg': 0.0, 'neu': 0.844, 'pos': 0.156, 'comp...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21998</th>\n",
       "      <td>India's COVID-19 Crisis Is Becoming a Global E...</td>\n",
       "      <td>5/12/2021 19:44</td>\n",
       "      <td>w7_ch</td>\n",
       "      <td>NaN</td>\n",
       "      <td>covid-19</td>\n",
       "      <td>{'neg': 0.528, 'neu': 0.472, 'pos': 0.0, 'comp...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21999</th>\n",
       "      <td>CM Shri , in a core group meeting, takes an im...</td>\n",
       "      <td>5/12/2021 19:44</td>\n",
       "      <td>sanjaythakor895</td>\n",
       "      <td>MORBi, Gujarat</td>\n",
       "      <td>covid-19</td>\n",
       "      <td>{'neg': 0.0, 'neu': 0.812, 'pos': 0.188, 'comp...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6399 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text             date  \\\n",
       "15601  Appointments for COVID-19 vaccinations are ava...  5/11/2021 16:58   \n",
       "15602  *COVID-19 TESTING IN SOUTH TRINIDAD* A drive-i...  5/11/2021 16:58   \n",
       "15603  Uber, Lyft to Provide Free Rides to Covid-19 V...  5/11/2021 16:58   \n",
       "15604  In order to maintain public health and safety ...  5/11/2021 16:58   \n",
       "15605  #Goa's decision to administer #ivermectin to a...  5/11/2021 16:58   \n",
       "...                                                  ...              ...   \n",
       "21995  CIOs and CTOs say they adapted well to disrupt...  5/12/2021 19:44   \n",
       "21996  U.S. health advisors endorsed the use of Pfize...  5/12/2021 19:44   \n",
       "21997  Fact-checking the Pandemic: the Truth about CO...  5/12/2021 19:44   \n",
       "21998  India's COVID-19 Crisis Is Becoming a Global E...  5/12/2021 19:44   \n",
       "21999  CM Shri , in a core group meeting, takes an im...  5/12/2021 19:44   \n",
       "\n",
       "                  user             location   keyword  \\\n",
       "15601     mcculloughmg        Kalamazoo, MI  covid-19   \n",
       "15602         KyJuve10  Trinidad and Tobago  covid-19   \n",
       "15603  masonis_marilyn        Michigan, USA  covid-19   \n",
       "15604  sarcast87934056                 Oman  covid-19   \n",
       "15605         r_umeash        Satara, India  covid-19   \n",
       "...                ...                  ...       ...   \n",
       "21995          neuzida               Remote  covid-19   \n",
       "21996         News12BK         Brooklyn, NY  covid-19   \n",
       "21997         real_gtk                  NaN  covid-19   \n",
       "21998            w7_ch                  NaN  covid-19   \n",
       "21999  sanjaythakor895       MORBi, Gujarat  covid-19   \n",
       "\n",
       "                                                  scores     label  \n",
       "15601  {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...   Neutral  \n",
       "15602  {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...   Neutral  \n",
       "15603  {'neg': 0.0, 'neu': 0.784, 'pos': 0.216, 'comp...  Positive  \n",
       "15604  {'neg': 0.092, 'neu': 0.792, 'pos': 0.117, 'co...  Positive  \n",
       "15605  {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...   Neutral  \n",
       "...                                                  ...       ...  \n",
       "21995  {'neg': 0.145, 'neu': 0.727, 'pos': 0.127, 'co...  Negative  \n",
       "21996  {'neg': 0.0, 'neu': 0.909, 'pos': 0.091, 'comp...  Positive  \n",
       "21997  {'neg': 0.0, 'neu': 0.844, 'pos': 0.156, 'comp...  Positive  \n",
       "21998  {'neg': 0.528, 'neu': 0.472, 'pos': 0.0, 'comp...  Negative  \n",
       "21999  {'neg': 0.0, 'neu': 0.812, 'pos': 0.188, 'comp...  Positive  \n",
       "\n",
       "[6399 rows x 7 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df[15601:22000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for converting emoticons to text\n",
    "emoticons = {\n",
    "':-)': 'happy / smile', \n",
    "':)': 'happy / smile', \n",
    "';)': 'wink / glad', \n",
    "':o)': 'happy / smile', \n",
    "':]': 'happy / smile', \n",
    "':3': 'happy / smile', \n",
    "':c)': 'happy / smile',\n",
    "':>': 'happy / smile', \n",
    "'=]': 'happy / smile', \n",
    "'8)': 'happy / smile', \n",
    "'=)': 'happy / smile', \n",
    "':}': 'happy / smile',\n",
    "':^)': 'happy / smile', \n",
    "':-D': 'laugh / big grin',\n",
    "':D': 'laugh / big grin',\n",
    "'8-D': 'laugh / big grin / laugh with glasses / wide-eyed surprise', \n",
    "'8D': 'laugh / big grin / laugh with glasses / wide-eyed surprise', \n",
    "'x-D': 'laugh', \n",
    "'xD': 'laugh', \n",
    "'X-D': 'laugh', \n",
    "'XD': 'laugh', \n",
    "'=-D': 'laugh / big grin', \n",
    "'=D': 'laugh / big grin',\n",
    "'=-3': 'laugh / big grin', \n",
    "'=3': 'laugh / big grin', \n",
    "':-))': 'very happy / double chin', \n",
    "\":'-)\": 'tears of happiness', \n",
    "\":')\": 'tears of happiness', \n",
    "':*': 'kiss', \n",
    "':^*': 'kiss', \n",
    "'>:P': 'tongue sticking out / cheeky / playful / blowing a raspberry', \n",
    "':-P': 'tongue sticking out / cheeky / playful / blowing a raspberry', \n",
    "':P': 'tongue sticking out / cheeky / playful / blowing a raspberry', \n",
    "'X-P': 'tongue sticking out / cheeky / playful / blowing a raspberry',\n",
    "'x-p': 'tongue sticking out / cheeky / playful / blowing a raspberry', \n",
    "'xp': 'tongue sticking out / cheeky / playful / blowing a raspberry', \n",
    "'XP': 'tongue sticking out / cheeky / playful / blowing a raspberry', \n",
    "':-p': 'tongue sticking out / cheeky / playful / blowing a raspberry', \n",
    "':p': 'tongue sticking out / cheeky / playful / blowing a raspberry', \n",
    "'=p': 'tongue sticking out / cheeky / playful / blowing a raspberry', \n",
    "':-b': 'tongue sticking out / cheeky / playful / blowing a raspberry', \n",
    "':b': 'tongue sticking out / cheeky / playful / blowing a raspberry', \n",
    "'>:)': 'devilish / cheeky / playful', \n",
    "'>;)': 'devilish / cheeky / playful / wink', \n",
    "'>:-)': 'devilish / cheeky / playful',\n",
    "'<3': 'heart / love',\n",
    "':L': 'skeptical / undecided / uneasy / hesitant', \n",
    "':-/': 'skeptical / undecided / uneasy / hesitant', \n",
    "'>:/': 'skeptical / annoyed / undecided / uneasy / hesitant', \n",
    "':S': 'skeptical / undecided / uneasy / hesitant', \n",
    "'>:[': 'frown / angry / pouting', \n",
    "':@': 'frown / sad / pouting', \n",
    "':-(': 'frown / sad / pouting', \n",
    "':[': 'frown / sad / pouting', \n",
    "':-||': 'frown / pouting', \n",
    "'=L': 'skeptical / undecided / uneasy / hesitant', \n",
    "':<': 'frown / sad / pouting',\n",
    "':-[': 'frown / sad / pouting', \n",
    "':-<': 'frown / sad / pouting', \n",
    "'=\\\\': 'skeptical / undecided / uneasy / hesitant', \n",
    "'=/': 'skeptical / undecided / uneasy / hesitant', \n",
    "'>:(': 'skeptical / annoyed / undecided / uneasy / hesitant', \n",
    "':(': 'frown / sad / pouting', \n",
    "'>.<': 'frown / pouting', \n",
    "\":'-(\": 'cry', \n",
    "\":'(\": 'cry', \n",
    "':\\\\': 'skeptical / undecided / uneasy / hesitant', \n",
    "':-c': 'frown / sad / pouting',\n",
    "':c': 'frown / sad / pouting', \n",
    "':{': 'frown / sad / pouting', \n",
    "'>:\\\\': 'skeptical / annoyed / undecided / uneasy / hesitant', \n",
    "';(': 'skeptical / annoyed / undecided / uneasy / hesitant'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def emoticon_text(text):\n",
    "    words=text.split()\n",
    "    reformed = [emoticons[word] if word in emoticons else word for word in words]\n",
    "    text = \" \".join(reformed)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apply converting emoticons to text function to text column in df_tweets dataframe\n",
    "new_df['cleaned_text'] = new_df['text'].apply(lambda x: emoticon_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>cleaned_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15600</th>\n",
       "      <td>In Nepal, a situation is unfolding that looks ...</td>\n",
       "      <td>In Nepal, a situation is unfolding that looks ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15601</th>\n",
       "      <td>Appointments for COVID-19 vaccinations are ava...</td>\n",
       "      <td>Appointments for COVID-19 vaccinations are ava...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15602</th>\n",
       "      <td>*COVID-19 TESTING IN SOUTH TRINIDAD* A drive-i...</td>\n",
       "      <td>*COVID-19 TESTING IN SOUTH TRINIDAD* A drive-i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15603</th>\n",
       "      <td>Uber, Lyft to Provide Free Rides to Covid-19 V...</td>\n",
       "      <td>Uber, Lyft to Provide Free Rides to Covid-19 V...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15604</th>\n",
       "      <td>In order to maintain public health and safety ...</td>\n",
       "      <td>In order to maintain public health and safety ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29995</th>\n",
       "      <td>Test mein kaunsa Finisher bhai India rarely chaâ€¦</td>\n",
       "      <td>Test mein kaunsa Finisher bhai India rarely chaâ€¦</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29996</th>\n",
       "      <td>Our dream of pursuing higher studies is at sta...</td>\n",
       "      <td>Our dream of pursuing higher studies is at sta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29997</th>\n",
       "      <td>We've been in lockdown already for almost 8 mo...</td>\n",
       "      <td>We've been in lockdown already for almost 8 mo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29998</th>\n",
       "      <td>States have put lockdown as centre never inter...</td>\n",
       "      <td>States have put lockdown as centre never inter...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29999</th>\n",
       "      <td>More cutting edge journalism from Conservative...</td>\n",
       "      <td>More cutting edge journalism from Conservative...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14400 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  \\\n",
       "15600  In Nepal, a situation is unfolding that looks ...   \n",
       "15601  Appointments for COVID-19 vaccinations are ava...   \n",
       "15602  *COVID-19 TESTING IN SOUTH TRINIDAD* A drive-i...   \n",
       "15603  Uber, Lyft to Provide Free Rides to Covid-19 V...   \n",
       "15604  In order to maintain public health and safety ...   \n",
       "...                                                  ...   \n",
       "29995   Test mein kaunsa Finisher bhai India rarely chaâ€¦   \n",
       "29996  Our dream of pursuing higher studies is at sta...   \n",
       "29997  We've been in lockdown already for almost 8 mo...   \n",
       "29998  States have put lockdown as centre never inter...   \n",
       "29999  More cutting edge journalism from Conservative...   \n",
       "\n",
       "                                            cleaned_text  \n",
       "15600  In Nepal, a situation is unfolding that looks ...  \n",
       "15601  Appointments for COVID-19 vaccinations are ava...  \n",
       "15602  *COVID-19 TESTING IN SOUTH TRINIDAD* A drive-i...  \n",
       "15603  Uber, Lyft to Provide Free Rides to Covid-19 V...  \n",
       "15604  In order to maintain public health and safety ...  \n",
       "...                                                  ...  \n",
       "29995   Test mein kaunsa Finisher bhai India rarely chaâ€¦  \n",
       "29996  Our dream of pursuing higher studies is at sta...  \n",
       "29997  We've been in lockdown already for almost 8 mo...  \n",
       "29998  States have put lockdown as centre never inter...  \n",
       "29999  More cutting edge journalism from Conservative...  \n",
       "\n",
       "[14400 rows x 2 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df[['text','cleaned_text']][15600:30000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import emoji\n",
    "# function for twitter text cleaning \n",
    "def emoji_cleaner(text):\n",
    "# convert emojis to text\n",
    "    text = emoji.demojize(text)\n",
    "    text = text.replace(\":\",\" \")\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df['cleaned_text'] = new_df['cleaned_text'].apply(lambda x: emoji_cleaner(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>cleaned_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15600</th>\n",
       "      <td>In Nepal, a situation is unfolding that looks ...</td>\n",
       "      <td>In Nepal, a situation is unfolding that looks ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15601</th>\n",
       "      <td>Appointments for COVID-19 vaccinations are ava...</td>\n",
       "      <td>Appointments for COVID-19 vaccinations are ava...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15602</th>\n",
       "      <td>*COVID-19 TESTING IN SOUTH TRINIDAD* A drive-i...</td>\n",
       "      <td>*COVID-19 TESTING IN SOUTH TRINIDAD* A drive-i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15603</th>\n",
       "      <td>Uber, Lyft to Provide Free Rides to Covid-19 V...</td>\n",
       "      <td>Uber, Lyft to Provide Free Rides to Covid-19 V...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15604</th>\n",
       "      <td>In order to maintain public health and safety ...</td>\n",
       "      <td>In order to maintain public health and safety ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29995</th>\n",
       "      <td>Test mein kaunsa Finisher bhai India rarely chaâ€¦</td>\n",
       "      <td>Test mein kaunsa Finisher bhai India rarely chaâ€¦</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29996</th>\n",
       "      <td>Our dream of pursuing higher studies is at sta...</td>\n",
       "      <td>Our dream of pursuing higher studies is at sta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29997</th>\n",
       "      <td>We've been in lockdown already for almost 8 mo...</td>\n",
       "      <td>We've been in lockdown already for almost 8 mo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29998</th>\n",
       "      <td>States have put lockdown as centre never inter...</td>\n",
       "      <td>States have put lockdown as centre never inter...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29999</th>\n",
       "      <td>More cutting edge journalism from Conservative...</td>\n",
       "      <td>More cutting edge journalism from Conservative...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14400 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  \\\n",
       "15600  In Nepal, a situation is unfolding that looks ...   \n",
       "15601  Appointments for COVID-19 vaccinations are ava...   \n",
       "15602  *COVID-19 TESTING IN SOUTH TRINIDAD* A drive-i...   \n",
       "15603  Uber, Lyft to Provide Free Rides to Covid-19 V...   \n",
       "15604  In order to maintain public health and safety ...   \n",
       "...                                                  ...   \n",
       "29995   Test mein kaunsa Finisher bhai India rarely chaâ€¦   \n",
       "29996  Our dream of pursuing higher studies is at sta...   \n",
       "29997  We've been in lockdown already for almost 8 mo...   \n",
       "29998  States have put lockdown as centre never inter...   \n",
       "29999  More cutting edge journalism from Conservative...   \n",
       "\n",
       "                                            cleaned_text  \n",
       "15600  In Nepal, a situation is unfolding that looks ...  \n",
       "15601  Appointments for COVID-19 vaccinations are ava...  \n",
       "15602  *COVID-19 TESTING IN SOUTH TRINIDAD* A drive-i...  \n",
       "15603  Uber, Lyft to Provide Free Rides to Covid-19 V...  \n",
       "15604  In order to maintain public health and safety ...  \n",
       "...                                                  ...  \n",
       "29995   Test mein kaunsa Finisher bhai India rarely chaâ€¦  \n",
       "29996  Our dream of pursuing higher studies is at sta...  \n",
       "29997  We've been in lockdown already for almost 8 mo...  \n",
       "29998  States have put lockdown as centre never inter...  \n",
       "29999  More cutting edge journalism from Conservative...  \n",
       "\n",
       "[14400 rows x 2 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df[['text','cleaned_text']][15600:30000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Replace all acronyms with their translation\n",
    "\n",
    "Next, we replace all acronyms with their translation using the acronym dictionary. At this point, tweets are going to be tokenized by getting rid of the punctuation and using split in order to do the process really fast. We could use nltk.tokenizer but it is definitly much much slower (also much more accurate). Furthermore the replacements will not be perfect, a simple example is the acronym \"im\" meaning \"instant message\". It would not be surprising that in most of the cases, \"im\" means \"I am\". We will do some adjustements later to see if we can improve our results.|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Acronym</th>\n",
       "      <th>Translation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5459</th>\n",
       "      <td>tomoz</td>\n",
       "      <td>tomorrow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5460</th>\n",
       "      <td>gpytfaht</td>\n",
       "      <td>gladly pay you tuesday for a hamburger today</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5461</th>\n",
       "      <td>l8rz</td>\n",
       "      <td>later</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5462</th>\n",
       "      <td>sase</td>\n",
       "      <td>self addressed stamped envelope</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5463</th>\n",
       "      <td>bwoc</td>\n",
       "      <td>big woman on campus</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Acronym                                   Translation\n",
       "5459     tomoz                                      tomorrow\n",
       "5460  gpytfaht  gladly pay you tuesday for a hamburger today\n",
       "5461      l8rz                                         later\n",
       "5462      sase               self addressed stamped envelope\n",
       "5463      bwoc                           big woman on campus"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acronyms = pd.read_csv('C:\\\\Users\\\\user\\\\Twitter analysis Project\\\\tools\\\\acronyms.csv')\n",
    "acronyms.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "acronym_dictionary = dict(zip(acronyms.Acronym, acronyms.Translation))\n",
    "acronyms_counter = Counter()\n",
    "def acronym_translation(text,acronyms_counter):\n",
    "    words=text.split()\n",
    "    new_words=[]\n",
    "    for x,word in enumerate(words):\n",
    "        if word in acronym_dictionary:\n",
    "            acronyms_counter[word] += 1\n",
    "            new_words.extend(acronym_dictionary[word].split())\n",
    "\n",
    "        else:\n",
    "            new_words.append(word)\n",
    "    return \" \".join(new_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df['cleaned_text'] = new_df['cleaned_text'].apply(lambda tweet:acronym_translation(tweet, acronyms_counter)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('2', 542),\n",
       " ('4', 257),\n",
       " ('u', 254),\n",
       " ('gonna', 111),\n",
       " ('r', 95),\n",
       " ('dont', 86),\n",
       " ('ur', 77),\n",
       " ('ppl', 75),\n",
       " ('n', 74),\n",
       " ('im', 69),\n",
       " ('bc', 59),\n",
       " ('pls', 56),\n",
       " ('wanna', 53),\n",
       " ('hai', 49),\n",
       " ('lol', 47),\n",
       " ('yr', 47),\n",
       " ('app', 46),\n",
       " ('@', 43),\n",
       " ('plz', 42),\n",
       " ('w/', 39)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get and display top20 acronyms\n",
    "top20acronyms = acronyms_counter.most_common(20)\n",
    "top20acronyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1) 2 => too : 542\n",
      "2) 4 => for : 257\n",
      "3) u => you : 254\n",
      "4) gonna => going to : 111\n",
      "5) r => are : 95\n",
      "6) dont => don't : 86\n",
      "7) ur => your : 77\n",
      "8) ppl => people : 75\n",
      "9) n => and : 74\n",
      "10) im => instant message : 69\n",
      "11) bc => because : 59\n",
      "12) pls => please : 56\n",
      "13) wanna => want to : 53\n",
      "14) hai => hello : 49\n",
      "15) lol => laughing out loud : 47\n",
      "16) yr => year : 47\n",
      "17) app => application : 46\n",
      "18) @ => at : 43\n",
      "19) plz => please : 42\n",
      "20) w/ => with : 39\n"
     ]
    }
   ],
   "source": [
    "# Just to better visualize the top 20 acronym\n",
    "for i, (acronym, value) in enumerate(top20acronyms):\n",
    "    print( str(i + 1) + \") \" + acronym + \" => \" + acronym_dictionary[acronym] + \" : \" + str(value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([<matplotlib.axis.XTick at 0x2cca8822fa0>,\n",
       "  <matplotlib.axis.XTick at 0x2cca8822f70>,\n",
       "  <matplotlib.axis.XTick at 0x2cca8807be0>,\n",
       "  <matplotlib.axis.XTick at 0x2cca8f292b0>,\n",
       "  <matplotlib.axis.XTick at 0x2cca8f297c0>,\n",
       "  <matplotlib.axis.XTick at 0x2cca8f29cd0>,\n",
       "  <matplotlib.axis.XTick at 0x2cca8f29940>,\n",
       "  <matplotlib.axis.XTick at 0x2cca8f36220>,\n",
       "  <matplotlib.axis.XTick at 0x2cca8f36730>,\n",
       "  <matplotlib.axis.XTick at 0x2cca8f36c40>,\n",
       "  <matplotlib.axis.XTick at 0x2cca8f3a1c0>,\n",
       "  <matplotlib.axis.XTick at 0x2cca8f3a6d0>,\n",
       "  <matplotlib.axis.XTick at 0x2cca8f3abe0>,\n",
       "  <matplotlib.axis.XTick at 0x2cca8f3f130>,\n",
       "  <matplotlib.axis.XTick at 0x2cca8f3f640>,\n",
       "  <matplotlib.axis.XTick at 0x2cca8f3a760>,\n",
       "  <matplotlib.axis.XTick at 0x2cca8f367c0>,\n",
       "  <matplotlib.axis.XTick at 0x2cca8f3f970>,\n",
       "  <matplotlib.axis.XTick at 0x2cca8f3fc70>,\n",
       "  <matplotlib.axis.XTick at 0x2cca8f471c0>],\n",
       " [Text(0, 0, '2'),\n",
       "  Text(0, 0, '4'),\n",
       "  Text(0, 0, 'u'),\n",
       "  Text(0, 0, 'gonna'),\n",
       "  Text(0, 0, 'r'),\n",
       "  Text(0, 0, 'dont'),\n",
       "  Text(0, 0, 'ur'),\n",
       "  Text(0, 0, 'ppl'),\n",
       "  Text(0, 0, 'n'),\n",
       "  Text(0, 0, 'im'),\n",
       "  Text(0, 0, 'bc'),\n",
       "  Text(0, 0, 'pls'),\n",
       "  Text(0, 0, 'wanna'),\n",
       "  Text(0, 0, 'hai'),\n",
       "  Text(0, 0, 'lol'),\n",
       "  Text(0, 0, 'yr'),\n",
       "  Text(0, 0, 'app'),\n",
       "  Text(0, 0, '@'),\n",
       "  Text(0, 0, 'plz'),\n",
       "  Text(0, 0, 'w/')])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# With a bar plot\n",
    "plt.close()\n",
    "top20acronym_keys = [x[0] for x in top20acronyms]\n",
    "top20acronym_values = [x[1] for x in top20acronyms]\n",
    "indexes = np.arange(len(top20acronym_keys))\n",
    "width = 0.7\n",
    "plt.bar(indexes, top20acronym_values, width)\n",
    "plt.xticks(indexes + width * 0.5, top20acronym_keys, rotation=\"vertical\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for replacing contractions\n",
    "contractions = { \n",
    "\"ain't\": \"am not / are not / is not / has not / have not\",\n",
    "\"aren't\": \"are not / am not\",\n",
    "\"can't\": \"cannot\",\n",
    "\"can't've\": \"cannot have\",\n",
    "\"'cause\": \"because\",\n",
    "\"could've\": \"could have\",\n",
    "\"couldn't\": \"could not\",\n",
    "\"couldn't've\": \"could not have\",\n",
    "\"didn't\": \"did not\",\n",
    "\"doesn't\": \"does not\",\n",
    "\"don't\": \"do not\",\n",
    "\"hadn't\": \"had not\",\n",
    "\"hadn't've\": \"had not have\",\n",
    "\"hasn't\": \"has not\",\n",
    "\"haven't\": \"have not\",\n",
    "\"he'd\": \"he had / he would\",\n",
    "\"he'd've\": \"he would have\",\n",
    "\"he'll\": \"he shall / he will\",\n",
    "\"he'll've\": \"he shall have / he will have\",\n",
    "\"he's\": \"he has / he is\",\n",
    "\"how'd\": \"how did\",\n",
    "\"how'd'y\": \"how do you\",\n",
    "\"how'll\": \"how will\",\n",
    "\"how's\": \"how has / how is / how does\",\n",
    "\"i'd\": \"i had / i would\",\n",
    "\"i'd've\": \"i would have\",\n",
    "\"i'll\": \"i shall / i will\",\n",
    "\"i'll've\": \"i shall have / i will have\",\n",
    "\"i'm\": \"i am\",\n",
    "\"i've\": \"i have\",\n",
    "\"isn't\": \"is not\",\n",
    "\"it'd\": \"it had / it would\",\n",
    "\"it'd've\": \"it would have\",\n",
    "\"it'll\": \"it shall / it will\",\n",
    "\"it'll've\": \"it shall have / it will have\",\n",
    "\"it's\": \"it has / it is\",\n",
    "\"let's\": \"let us\",\n",
    "\"ma'am\": \"madam\",\n",
    "\"mayn't\": \"may not\",\n",
    "\"might've\": \"might have\",\n",
    "\"mightn't\": \"might not\",\n",
    "\"mightn't've\": \"might not have\",\n",
    "\"must've\": \"must have\",\n",
    "\"mustn't\": \"must not\",\n",
    "\"mustn't've\": \"must not have\",\n",
    "\"needn't\": \"need not\",\n",
    "\"needn't've\": \"need not have\",\n",
    "\"o'clock\": \"of the clock\",\n",
    "\"oughtn't\": \"ought not\",\n",
    "\"oughtn't've\": \"ought not have\",\n",
    "\"shan't\": \"shall not\",\n",
    "\"sha'n't\": \"shall not\",\n",
    "\"shan't've\": \"shall not have\",\n",
    "\"she'd\": \"she had / she would\",\n",
    "\"she'd've\": \"she would have\",\n",
    "\"she'll\": \"she shall / she will\",\n",
    "\"she'll've\": \"she shall have / she will have\",\n",
    "\"she's\": \"she has / she is\",\n",
    "\"should've\": \"should have\",\n",
    "\"shouldn't\": \"should not\",\n",
    "\"shouldn't've\": \"should not have\",\n",
    "\"so've\": \"so have\",\n",
    "\"so's\": \"so as / so is\",\n",
    "\"that'd\": \"that would / that had\",\n",
    "\"that'd've\": \"that would have\",\n",
    "\"that's\": \"that has / that is\",\n",
    "\"there'd\": \"there had / there would\",\n",
    "\"there'd've\": \"there would have\",\n",
    "\"there's\": \"there has / there is\",\n",
    "\"they'd\": \"they had / they would\",\n",
    "\"they'd've\": \"they would have\",\n",
    "\"they'll\": \"they shall / they will\",\n",
    "\"they'll've\": \"they shall have / they will have\",\n",
    "\"they're\": \"they are\",\n",
    "\"they've\": \"they have\",\n",
    "\"to've\": \"to have\",\n",
    "\"wasn't\": \"was not\",\n",
    "\"we'd\": \"we had / we would\",\n",
    "\"we'd've\": \"we would have\",\n",
    "\"we'll\": \"we will\",\n",
    "\"we'll've\": \"we will have\",\n",
    "\"we're\": \"we are\",\n",
    "\"we've\": \"we have\",\n",
    "\"weren't\": \"were not\",\n",
    "\"what'll\": \"what shall / what will\",\n",
    "\"what'll've\": \"what shall have / what will have\",\n",
    "\"what're\": \"what are\",\n",
    "\"what's\": \"what has / what is\",\n",
    "\"what've\": \"what have\",\n",
    "\"when's\": \"when has / when is\",\n",
    "\"when've\": \"when have\",\n",
    "\"where'd\": \"where did\",\n",
    "\"where's\": \"where has / where is\",\n",
    "\"where've\": \"where have\",\n",
    "\"who'll\": \"who shall / who will\",\n",
    "\"who'll've\": \"who shall have / who will have\",\n",
    "\"who's\": \"who has / who is\",\n",
    "\"who've\": \"who have\",\n",
    "\"why's\": \"why has / why is\",\n",
    "\"why've\": \"why have\",\n",
    "\"will've\": \"will have\",\n",
    "\"won't\": \"will not\",\n",
    "\"won't've\": \"will not have\",\n",
    "\"would've\": \"would have\",\n",
    "\"wouldn't\": \"would not\",\n",
    "\"wouldn't've\": \"would not have\",\n",
    "\"y'all\": \"you all\",\n",
    "\"y'all'd\": \"you all would\",\n",
    "\"y'all'd've\": \"you all would have\",\n",
    "\"y'all're\": \"you all are\",\n",
    "\"y'all've\": \"you all have\",\n",
    "\"you'd\": \"you had / you would\",\n",
    "\"you'd've\": \"you would have\",\n",
    "\"you'll\": \"you shall / you will\",\n",
    "\"you'll've\": \"you shall have / you will have\",\n",
    "\"you're\": \"you are\",\n",
    "\"you've\": \"you have\"\n",
    "}\n",
    "\n",
    "def replace_contractions(text):\n",
    "    text = text.replace(\"â€™\",\"'\")\n",
    "    text=text.lower()\n",
    "    words = text.split()\n",
    "    reformed = [contractions[word] if word in contractions else word for word in words]\n",
    "    text = \" \".join(reformed)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apply replace contractions function to cleaned_text column in df_tweets dataframe\n",
    "new_df['cleaned_text'] = new_df['cleaned_text'].apply(lambda x: replace_contractions(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>cleaned_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>355</th>\n",
       "      <td>COVID-19 vaccine rollout by India has 'rescued...</td>\n",
       "      <td>covid-19 vaccine rollout by india has 'rescued...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>356</th>\n",
       "      <td>No vaccine in our state#jharkhand</td>\n",
       "      <td>no vaccine in our state#jharkhand</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>357</th>\n",
       "      <td>â€œI call upon the US to put an end to export ba...</td>\n",
       "      <td>â€œi call upon the us to put an end to export ba...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>358</th>\n",
       "      <td>Me when others ask if I have had my vaccine ei...</td>\n",
       "      <td>me when others ask if i have had my vaccine ei...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>359</th>\n",
       "      <td>Fact check: No evidence that a 2-year-old died...</td>\n",
       "      <td>fact check no evidence that a 2-year-old died ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29995</th>\n",
       "      <td>Test mein kaunsa Finisher bhai India rarely chaâ€¦</td>\n",
       "      <td>test mein kaunsa finisher bhai india rarely chaâ€¦</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29996</th>\n",
       "      <td>Our dream of pursuing higher studies is at sta...</td>\n",
       "      <td>our dream of pursuing higher studies is at sta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29997</th>\n",
       "      <td>We've been in lockdown already for almost 8 mo...</td>\n",
       "      <td>we have been in lockdown already for almost 8 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29998</th>\n",
       "      <td>States have put lockdown as centre never inter...</td>\n",
       "      <td>states have put lockdown as centre never inter...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29999</th>\n",
       "      <td>More cutting edge journalism from Conservative...</td>\n",
       "      <td>more cutting edge journalism from conservative...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>29645 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  \\\n",
       "355    COVID-19 vaccine rollout by India has 'rescued...   \n",
       "356                    No vaccine in our state#jharkhand   \n",
       "357    â€œI call upon the US to put an end to export ba...   \n",
       "358    Me when others ask if I have had my vaccine ei...   \n",
       "359    Fact check: No evidence that a 2-year-old died...   \n",
       "...                                                  ...   \n",
       "29995   Test mein kaunsa Finisher bhai India rarely chaâ€¦   \n",
       "29996  Our dream of pursuing higher studies is at sta...   \n",
       "29997  We've been in lockdown already for almost 8 mo...   \n",
       "29998  States have put lockdown as centre never inter...   \n",
       "29999  More cutting edge journalism from Conservative...   \n",
       "\n",
       "                                            cleaned_text  \n",
       "355    covid-19 vaccine rollout by india has 'rescued...  \n",
       "356                    no vaccine in our state#jharkhand  \n",
       "357    â€œi call upon the us to put an end to export ba...  \n",
       "358    me when others ask if i have had my vaccine ei...  \n",
       "359    fact check no evidence that a 2-year-old died ...  \n",
       "...                                                  ...  \n",
       "29995   test mein kaunsa finisher bhai india rarely chaâ€¦  \n",
       "29996  our dream of pursuing higher studies is at sta...  \n",
       "29997  we have been in lockdown already for almost 8 ...  \n",
       "29998  states have put lockdown as centre never inter...  \n",
       "29999  more cutting edge journalism from conservative...  \n",
       "\n",
       "[29645 rows x 2 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df[['text','cleaned_text']][355:30000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Removing punctuations,numbers,special characters , whitespaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "# function for twitter text cleaning \n",
    "def tweet_cleaner(text):\n",
    "    # remove punctuations, numbers, and special characters \n",
    "    text = re.sub(\"[^a-zA-Z]\",\" \",text) \n",
    "        # remove whitespaces \n",
    "    text = ' '.join(text.split()) \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df['cleaned_text'] = new_df['cleaned_text'].apply(lambda x: tweet_cleaner(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>cleaned_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15600</th>\n",
       "      <td>In Nepal, a situation is unfolding that looks ...</td>\n",
       "      <td>in nepal a situation is unfolding that looks c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15601</th>\n",
       "      <td>Appointments for COVID-19 vaccinations are ava...</td>\n",
       "      <td>appointments for covid vaccinations are availa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15602</th>\n",
       "      <td>*COVID-19 TESTING IN SOUTH TRINIDAD* A drive-i...</td>\n",
       "      <td>covid testing in south trinidad a drive in cov...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15603</th>\n",
       "      <td>Uber, Lyft to Provide Free Rides to Covid-19 V...</td>\n",
       "      <td>uber lyft to provide free rides to covid vacci...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15604</th>\n",
       "      <td>In order to maintain public health and safety ...</td>\n",
       "      <td>in order to maintain public health and safety ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29995</th>\n",
       "      <td>Test mein kaunsa Finisher bhai India rarely chaâ€¦</td>\n",
       "      <td>test mein kaunsa finisher bhai india rarely cha</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29996</th>\n",
       "      <td>Our dream of pursuing higher studies is at sta...</td>\n",
       "      <td>our dream of pursuing higher studies is at sta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29997</th>\n",
       "      <td>We've been in lockdown already for almost 8 mo...</td>\n",
       "      <td>we have been in lockdown already for almost mo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29998</th>\n",
       "      <td>States have put lockdown as centre never inter...</td>\n",
       "      <td>states have put lockdown as centre never inter...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29999</th>\n",
       "      <td>More cutting edge journalism from Conservative...</td>\n",
       "      <td>more cutting edge journalism from conservative...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14400 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  \\\n",
       "15600  In Nepal, a situation is unfolding that looks ...   \n",
       "15601  Appointments for COVID-19 vaccinations are ava...   \n",
       "15602  *COVID-19 TESTING IN SOUTH TRINIDAD* A drive-i...   \n",
       "15603  Uber, Lyft to Provide Free Rides to Covid-19 V...   \n",
       "15604  In order to maintain public health and safety ...   \n",
       "...                                                  ...   \n",
       "29995   Test mein kaunsa Finisher bhai India rarely chaâ€¦   \n",
       "29996  Our dream of pursuing higher studies is at sta...   \n",
       "29997  We've been in lockdown already for almost 8 mo...   \n",
       "29998  States have put lockdown as centre never inter...   \n",
       "29999  More cutting edge journalism from Conservative...   \n",
       "\n",
       "                                            cleaned_text  \n",
       "15600  in nepal a situation is unfolding that looks c...  \n",
       "15601  appointments for covid vaccinations are availa...  \n",
       "15602  covid testing in south trinidad a drive in cov...  \n",
       "15603  uber lyft to provide free rides to covid vacci...  \n",
       "15604  in order to maintain public health and safety ...  \n",
       "...                                                  ...  \n",
       "29995    test mein kaunsa finisher bhai india rarely cha  \n",
       "29996  our dream of pursuing higher studies is at sta...  \n",
       "29997  we have been in lockdown already for almost mo...  \n",
       "29998  states have put lockdown as centre never inter...  \n",
       "29999  more cutting edge journalism from conservative...  \n",
       "\n",
       "[14400 rows x 2 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df[['text','cleaned_text']][15600:30000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer \n",
    "from nltk.stem.wordnet import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# function for lemmatizing words\n",
    "def lemmatize_text(text):\n",
    "    wl = WordNetLemmatizer()\n",
    "    token_words=word_tokenize(str(text))\n",
    "    token_words\n",
    "    lemmatize_text=[]\n",
    "    for word in token_words:\n",
    "        lemmatize_text.append(wl.lemmatize(word))\n",
    "        lemmatize_text.append(\" \")\n",
    "    return \"\".join(lemmatize_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Apply lemmatizing text function to cleaned_text column in df_tweets dataframe\n",
    "new_df['cleaned_text'] = new_df['cleaned_text'].apply(lambda x: lemmatize_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>cleaned_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15600</th>\n",
       "      <td>In Nepal, a situation is unfolding that looks ...</td>\n",
       "      <td>in nepal a situation is unfolding that look ch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15601</th>\n",
       "      <td>Appointments for COVID-19 vaccinations are ava...</td>\n",
       "      <td>appointment for covid vaccination are availabl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15602</th>\n",
       "      <td>*COVID-19 TESTING IN SOUTH TRINIDAD* A drive-i...</td>\n",
       "      <td>covid testing in south trinidad a drive in cov...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15603</th>\n",
       "      <td>Uber, Lyft to Provide Free Rides to Covid-19 V...</td>\n",
       "      <td>uber lyft to provide free ride to covid vaccin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15604</th>\n",
       "      <td>In order to maintain public health and safety ...</td>\n",
       "      <td>in order to maintain public health and safety ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29995</th>\n",
       "      <td>Test mein kaunsa Finisher bhai India rarely chaâ€¦</td>\n",
       "      <td>test mein kaunsa finisher bhai india rarely cha</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29996</th>\n",
       "      <td>Our dream of pursuing higher studies is at sta...</td>\n",
       "      <td>our dream of pursuing higher study is at stake...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29997</th>\n",
       "      <td>We've been in lockdown already for almost 8 mo...</td>\n",
       "      <td>we have been in lockdown already for almost mo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29998</th>\n",
       "      <td>States have put lockdown as centre never inter...</td>\n",
       "      <td>state have put lockdown a centre never interfe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29999</th>\n",
       "      <td>More cutting edge journalism from Conservative...</td>\n",
       "      <td>more cutting edge journalism from conservative...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14400 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  \\\n",
       "15600  In Nepal, a situation is unfolding that looks ...   \n",
       "15601  Appointments for COVID-19 vaccinations are ava...   \n",
       "15602  *COVID-19 TESTING IN SOUTH TRINIDAD* A drive-i...   \n",
       "15603  Uber, Lyft to Provide Free Rides to Covid-19 V...   \n",
       "15604  In order to maintain public health and safety ...   \n",
       "...                                                  ...   \n",
       "29995   Test mein kaunsa Finisher bhai India rarely chaâ€¦   \n",
       "29996  Our dream of pursuing higher studies is at sta...   \n",
       "29997  We've been in lockdown already for almost 8 mo...   \n",
       "29998  States have put lockdown as centre never inter...   \n",
       "29999  More cutting edge journalism from Conservative...   \n",
       "\n",
       "                                            cleaned_text  \n",
       "15600  in nepal a situation is unfolding that look ch...  \n",
       "15601  appointment for covid vaccination are availabl...  \n",
       "15602  covid testing in south trinidad a drive in cov...  \n",
       "15603  uber lyft to provide free ride to covid vaccin...  \n",
       "15604  in order to maintain public health and safety ...  \n",
       "...                                                  ...  \n",
       "29995   test mein kaunsa finisher bhai india rarely cha   \n",
       "29996  our dream of pursuing higher study is at stake...  \n",
       "29997  we have been in lockdown already for almost mo...  \n",
       "29998  state have put lockdown a centre never interfe...  \n",
       "29999  more cutting edge journalism from conservative...  \n",
       "\n",
       "[14400 rows x 2 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df[['text','cleaned_text']][15600:30000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#Remove stop words in cleaned_text column\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "# function to remove stopwords\n",
    "def remove_stopwords(text):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    no_stopword_text = [w for w in str(text).split() if not w in stop_words]\n",
    "    return no_stopword_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apply the remove stop words function to cleaned_text column in df_tweets dataframe\n",
    "new_df['cleaned_text'] = new_df['cleaned_text'].apply(lambda x: remove_stopwords(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove rows containing missing values under the cleaned_text column \n",
    "new_df = new_df[new_df['cleaned_text'].notnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40541, 8)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>cleaned_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15600</th>\n",
       "      <td>In Nepal, a situation is unfolding that looks ...</td>\n",
       "      <td>[nepal, situation, unfolding, look, chillingly...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15601</th>\n",
       "      <td>Appointments for COVID-19 vaccinations are ava...</td>\n",
       "      <td>[appointment, covid, vaccination, available, a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15602</th>\n",
       "      <td>*COVID-19 TESTING IN SOUTH TRINIDAD* A drive-i...</td>\n",
       "      <td>[covid, testing, south, trinidad, drive, covid...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15603</th>\n",
       "      <td>Uber, Lyft to Provide Free Rides to Covid-19 V...</td>\n",
       "      <td>[uber, lyft, provide, free, ride, covid, vacci...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15604</th>\n",
       "      <td>In order to maintain public health and safety ...</td>\n",
       "      <td>[order, maintain, public, health, safety, avoi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29995</th>\n",
       "      <td>Test mein kaunsa Finisher bhai India rarely chaâ€¦</td>\n",
       "      <td>[test, mein, kaunsa, finisher, bhai, india, ra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29996</th>\n",
       "      <td>Our dream of pursuing higher studies is at sta...</td>\n",
       "      <td>[dream, pursuing, higher, study, stake, due, u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29997</th>\n",
       "      <td>We've been in lockdown already for almost 8 mo...</td>\n",
       "      <td>[lockdown, already, almost, month, straight, k...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29998</th>\n",
       "      <td>States have put lockdown as centre never inter...</td>\n",
       "      <td>[state, put, lockdown, centre, never, interfer...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29999</th>\n",
       "      <td>More cutting edge journalism from Conservative...</td>\n",
       "      <td>[cutting, edge, journalism, conservative, woma...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14400 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  \\\n",
       "15600  In Nepal, a situation is unfolding that looks ...   \n",
       "15601  Appointments for COVID-19 vaccinations are ava...   \n",
       "15602  *COVID-19 TESTING IN SOUTH TRINIDAD* A drive-i...   \n",
       "15603  Uber, Lyft to Provide Free Rides to Covid-19 V...   \n",
       "15604  In order to maintain public health and safety ...   \n",
       "...                                                  ...   \n",
       "29995   Test mein kaunsa Finisher bhai India rarely chaâ€¦   \n",
       "29996  Our dream of pursuing higher studies is at sta...   \n",
       "29997  We've been in lockdown already for almost 8 mo...   \n",
       "29998  States have put lockdown as centre never inter...   \n",
       "29999  More cutting edge journalism from Conservative...   \n",
       "\n",
       "                                            cleaned_text  \n",
       "15600  [nepal, situation, unfolding, look, chillingly...  \n",
       "15601  [appointment, covid, vaccination, available, a...  \n",
       "15602  [covid, testing, south, trinidad, drive, covid...  \n",
       "15603  [uber, lyft, provide, free, ride, covid, vacci...  \n",
       "15604  [order, maintain, public, health, safety, avoi...  \n",
       "...                                                  ...  \n",
       "29995  [test, mein, kaunsa, finisher, bhai, india, ra...  \n",
       "29996  [dream, pursuing, higher, study, stake, due, u...  \n",
       "29997  [lockdown, already, almost, month, straight, k...  \n",
       "29998  [state, put, lockdown, centre, never, interfer...  \n",
       "29999  [cutting, edge, journalism, conservative, woma...  \n",
       "\n",
       "[14400 rows x 2 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df[['text','cleaned_text']][15600:30000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.to_csv(r'C:\\\\Users\\\\user\\\\Twitter analysis Project\\\\data\\\\processed_tweetsjuly20.csv', index = False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
