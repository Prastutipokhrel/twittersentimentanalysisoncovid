{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in d:\\programdata\\anaconda3\\lib\\site-packages (1.18.5)\n",
      "Requirement already satisfied: nltk in d:\\programdata\\anaconda3\\lib\\site-packages (3.5)\n",
      "Requirement already satisfied: joblib in d:\\programdata\\anaconda3\\lib\\site-packages (from nltk) (0.16.0)\n",
      "Requirement already satisfied: regex in d:\\programdata\\anaconda3\\lib\\site-packages (from nltk) (2020.6.8)\n",
      "Requirement already satisfied: click in d:\\programdata\\anaconda3\\lib\\site-packages (from nltk) (7.1.2)\n",
      "Requirement already satisfied: tqdm in d:\\programdata\\anaconda3\\lib\\site-packages (from nltk) (4.47.0)\n",
      "Requirement already satisfied: emoji in d:\\programdata\\anaconda3\\lib\\site-packages (1.2.0)\n",
      "Requirement already satisfied: pandas in d:\\programdata\\anaconda3\\lib\\site-packages (1.0.5)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in d:\\programdata\\anaconda3\\lib\\site-packages (from pandas) (2.8.1)\n",
      "Requirement already satisfied: numpy>=1.13.3 in d:\\programdata\\anaconda3\\lib\\site-packages (from pandas) (1.18.5)\n",
      "Requirement already satisfied: pytz>=2017.2 in d:\\programdata\\anaconda3\\lib\\site-packages (from pandas) (2020.1)\n",
      "Requirement already satisfied: six>=1.5 in d:\\programdata\\anaconda3\\lib\\site-packages (from python-dateutil>=2.6.1->pandas) (1.15.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy\n",
    "!pip install nltk\n",
    "!pip install emoji\n",
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import emoji\n",
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer \n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory C:\\Users\\user\\Untitled Folder 3\n",
      "Directory changed successfully E:\\twitter_SA\\tweets\n"
     ]
    }
   ],
   "source": [
    "path = r\"E:\\twitter_SA\\tweets\"\n",
    "\n",
    "# Check current working directory.\n",
    "retval = os.getcwd()\n",
    "print(\"Current working directory %s\" % retval)\n",
    "\n",
    "# Now change the directory\n",
    "os.chdir(path)\n",
    "\n",
    "# Check current working directory.\n",
    "retval = os.getcwd()\n",
    "\n",
    "print(\"Directory changed successfully %s\" % retval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use glob to match the pattern â€˜csvâ€™\n",
    "extension = 'csv'\n",
    "all_filenames = [i for i in glob.glob('*.{}'.format(extension))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#combine all files in the list\n",
    "df_tweets = pd.concat([pd.read_csv(f) for f in all_filenames ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SN</th>\n",
       "      <th>Text</th>\n",
       "      <th>Username</th>\n",
       "      <th>Location</th>\n",
       "      <th>Date</th>\n",
       "      <th>Topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>I am having a hard time with #Lent this year ...</td>\n",
       "      <td>wagagaikoko</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3/10/2021 19:10</td>\n",
       "      <td>corona</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>#Ø§Ù„Ø·Ø±ÙŠÙ‚_Ù…Ù‚Ø·ÙˆØ¹Citizens in Lebanon, are facing ...</td>\n",
       "      <td>RZgheib7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3/10/2021 19:10</td>\n",
       "      <td>corona</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>I had the Corona already last January. So who...</td>\n",
       "      <td>cestlaviv</td>\n",
       "      <td>Carinthia - Austria</td>\n",
       "      <td>3/10/2021 19:10</td>\n",
       "      <td>corona</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Ooohh karma, you wonderful thing,man's restric...</td>\n",
       "      <td>FlavionsKimani</td>\n",
       "      <td>Nairobi Kenya</td>\n",
       "      <td>3/10/2021 19:10</td>\n",
       "      <td>corona</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>#Ø§Ù„Ø·Ø±ÙŠÙ‚_Ù…Ù‚Ø·ÙˆØ¹Citizens in Lebanon, are facing ...</td>\n",
       "      <td>Zizo90693787</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3/10/2021 19:10</td>\n",
       "      <td>corona</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   SN                                               Text        Username  \\\n",
       "0   0   I am having a hard time with #Lent this year ...     wagagaikoko   \n",
       "1   1   #Ø§Ù„Ø·Ø±ÙŠÙ‚_Ù…Ù‚Ø·ÙˆØ¹Citizens in Lebanon, are facing ...        RZgheib7   \n",
       "2   2   I had the Corona already last January. So who...       cestlaviv   \n",
       "3   3  Ooohh karma, you wonderful thing,man's restric...  FlavionsKimani   \n",
       "4   4   #Ø§Ù„Ø·Ø±ÙŠÙ‚_Ù…Ù‚Ø·ÙˆØ¹Citizens in Lebanon, are facing ...    Zizo90693787   \n",
       "\n",
       "               Location             Date   Topic  \n",
       "0                   NaN  3/10/2021 19:10  corona  \n",
       "1                   NaN  3/10/2021 19:10  corona  \n",
       "2  Carinthia - Austria   3/10/2021 19:10  corona  \n",
       "3        Nairobi Kenya   3/10/2021 19:10  corona  \n",
       "4                   NaN  3/10/2021 19:10  corona  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check first 5 rows of df_tweets dataset\n",
    "df_tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23000, 6)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tweets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 23000 entries, 0 to 4999\n",
      "Data columns (total 6 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   SN        23000 non-null  int64 \n",
      " 1   Text      23000 non-null  object\n",
      " 2   Username  23000 non-null  object\n",
      " 3   Location  15126 non-null  object\n",
      " 4   Date      23000 non-null  object\n",
      " 5   Topic     23000 non-null  object\n",
      "dtypes: int64(1), object(5)\n",
      "memory usage: 1.2+ MB\n"
     ]
    }
   ],
   "source": [
    "#Get summary of df_tweets dataframe\n",
    "df_tweets.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SN             0\n",
       "Text           0\n",
       "Username       0\n",
       "Location    7874\n",
       "Date           0\n",
       "Topic          0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check sum of missing values in each column of df_tweets dataframe\n",
    "df_tweets.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Earliest tweet:  2021-03-03 14:10:00\n",
      "Latest tweet:  2021-03-11 17:24:00 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Convert created_at and account_created_at variables from object to datetime64 \n",
    "df_tweets['Date'] = df_tweets['Date'].astype('datetime64')\n",
    "# Check for earliest and latest tweet\n",
    "print(\"Earliest tweet: \", df_tweets['Date'].min())\n",
    "print(\"Latest tweet: \", df_tweets['Date'].max(), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for converting emoticons to text\n",
    "emoticons = {\n",
    "':-)': 'happy / smile', \n",
    "':)': 'happy / smile', \n",
    "';)': 'wink / glad', \n",
    "':o)': 'happy / smile', \n",
    "':]': 'happy / smile', \n",
    "':3': 'happy / smile', \n",
    "':c)': 'happy / smile',\n",
    "':>': 'happy / smile', \n",
    "'=]': 'happy / smile', \n",
    "'8)': 'happy / smile', \n",
    "'=)': 'happy / smile', \n",
    "':}': 'happy / smile',\n",
    "':^)': 'happy / smile', \n",
    "':-D': 'laugh / big grin',\n",
    "':D': 'laugh / big grin',\n",
    "'8-D': 'laugh / big grin / laugh with glasses / wide-eyed surprise', \n",
    "'8D': 'laugh / big grin / laugh with glasses / wide-eyed surprise', \n",
    "'x-D': 'laugh', \n",
    "'xD': 'laugh', \n",
    "'X-D': 'laugh', \n",
    "'XD': 'laugh', \n",
    "'=-D': 'laugh / big grin', \n",
    "'=D': 'laugh / big grin',\n",
    "'=-3': 'laugh / big grin', \n",
    "'=3': 'laugh / big grin', \n",
    "':-))': 'very happy / double chin', \n",
    "\":'-)\": 'tears of happiness', \n",
    "\":')\": 'tears of happiness', \n",
    "':*': 'kiss', \n",
    "':^*': 'kiss', \n",
    "'>:P': 'tongue sticking out / cheeky / playful / blowing a raspberry', \n",
    "':-P': 'tongue sticking out / cheeky / playful / blowing a raspberry', \n",
    "':P': 'tongue sticking out / cheeky / playful / blowing a raspberry', \n",
    "'X-P': 'tongue sticking out / cheeky / playful / blowing a raspberry',\n",
    "'x-p': 'tongue sticking out / cheeky / playful / blowing a raspberry', \n",
    "'xp': 'tongue sticking out / cheeky / playful / blowing a raspberry', \n",
    "'XP': 'tongue sticking out / cheeky / playful / blowing a raspberry', \n",
    "':-p': 'tongue sticking out / cheeky / playful / blowing a raspberry', \n",
    "':p': 'tongue sticking out / cheeky / playful / blowing a raspberry', \n",
    "'=p': 'tongue sticking out / cheeky / playful / blowing a raspberry', \n",
    "':-b': 'tongue sticking out / cheeky / playful / blowing a raspberry', \n",
    "':b': 'tongue sticking out / cheeky / playful / blowing a raspberry', \n",
    "'>:)': 'devilish / cheeky / playful', \n",
    "'>;)': 'devilish / cheeky / playful / wink', \n",
    "'>:-)': 'devilish / cheeky / playful',\n",
    "'<3': 'heart / love',\n",
    "':L': 'skeptical / undecided / uneasy / hesitant', \n",
    "':-/': 'skeptical / undecided / uneasy / hesitant', \n",
    "'>:/': 'skeptical / annoyed / undecided / uneasy / hesitant', \n",
    "':S': 'skeptical / undecided / uneasy / hesitant', \n",
    "'>:[': 'frown / angry / pouting', \n",
    "':@': 'frown / sad / pouting', \n",
    "':-(': 'frown / sad / pouting', \n",
    "':[': 'frown / sad / pouting', \n",
    "':-||': 'frown / pouting', \n",
    "'=L': 'skeptical / undecided / uneasy / hesitant', \n",
    "':<': 'frown / sad / pouting',\n",
    "':-[': 'frown / sad / pouting', \n",
    "':-<': 'frown / sad / pouting', \n",
    "'=\\\\': 'skeptical / undecided / uneasy / hesitant', \n",
    "'=/': 'skeptical / undecided / uneasy / hesitant', \n",
    "'>:(': 'skeptical / annoyed / undecided / uneasy / hesitant', \n",
    "':(': 'frown / sad / pouting', \n",
    "'>.<': 'frown / pouting', \n",
    "\":'-(\": 'cry', \n",
    "\":'(\": 'cry', \n",
    "':\\\\': 'skeptical / undecided / uneasy / hesitant', \n",
    "':-c': 'frown / sad / pouting',\n",
    "':c': 'frown / sad / pouting', \n",
    "':{': 'frown / sad / pouting', \n",
    "'>:\\\\': 'skeptical / annoyed / undecided / uneasy / hesitant', \n",
    "';(': 'skeptical / annoyed / undecided / uneasy / hesitant'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def emoticon_text(text):\n",
    "    words=text.split()\n",
    "    reformed = [emoticons[word] if word in emoticons else word for word in words]\n",
    "    text = \" \".join(reformed)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apply converting emoticons to text function to text column in df_tweets dataframe\n",
    "df_tweets['cleaned_text'] = df_tweets['Text'].apply(lambda x: emoticon_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for replacing contractions\n",
    "contractions = { \n",
    "\"ain't\": \"am not / are not / is not / has not / have not\",\n",
    "\"aren't\": \"are not / am not\",\n",
    "\"can't\": \"cannot\",\n",
    "\"can't've\": \"cannot have\",\n",
    "\"'cause\": \"because\",\n",
    "\"could've\": \"could have\",\n",
    "\"couldn't\": \"could not\",\n",
    "\"couldn't've\": \"could not have\",\n",
    "\"didn't\": \"did not\",\n",
    "\"doesn't\": \"does not\",\n",
    "\"don't\": \"do not\",\n",
    "\"hadn't\": \"had not\",\n",
    "\"hadn't've\": \"had not have\",\n",
    "\"hasn't\": \"has not\",\n",
    "\"haven't\": \"have not\",\n",
    "\"he'd\": \"he had / he would\",\n",
    "\"he'd've\": \"he would have\",\n",
    "\"he'll\": \"he shall / he will\",\n",
    "\"he'll've\": \"he shall have / he will have\",\n",
    "\"he's\": \"he has / he is\",\n",
    "\"how'd\": \"how did\",\n",
    "\"how'd'y\": \"how do you\",\n",
    "\"how'll\": \"how will\",\n",
    "\"how's\": \"how has / how is / how does\",\n",
    "\"I'd\": \"I had / I would\",\n",
    "\"I'd've\": \"I would have\",\n",
    "\"I'll\": \"I shall / I will\",\n",
    "\"I'll've\": \"I shall have / I will have\",\n",
    "\"I'm\": \"I am\",\n",
    "\"I've\": \"I have\",\n",
    "\"isn't\": \"is not\",\n",
    "\"it'd\": \"it had / it would\",\n",
    "\"it'd've\": \"it would have\",\n",
    "\"it'll\": \"it shall / it will\",\n",
    "\"it'll've\": \"it shall have / it will have\",\n",
    "\"it's\": \"it has / it is\",\n",
    "\"let's\": \"let us\",\n",
    "\"ma'am\": \"madam\",\n",
    "\"mayn't\": \"may not\",\n",
    "\"might've\": \"might have\",\n",
    "\"mightn't\": \"might not\",\n",
    "\"mightn't've\": \"might not have\",\n",
    "\"must've\": \"must have\",\n",
    "\"mustn't\": \"must not\",\n",
    "\"mustn't've\": \"must not have\",\n",
    "\"needn't\": \"need not\",\n",
    "\"needn't've\": \"need not have\",\n",
    "\"o'clock\": \"of the clock\",\n",
    "\"oughtn't\": \"ought not\",\n",
    "\"oughtn't've\": \"ought not have\",\n",
    "\"shan't\": \"shall not\",\n",
    "\"sha'n't\": \"shall not\",\n",
    "\"shan't've\": \"shall not have\",\n",
    "\"she'd\": \"she had / she would\",\n",
    "\"she'd've\": \"she would have\",\n",
    "\"she'll\": \"she shall / she will\",\n",
    "\"she'll've\": \"she shall have / she will have\",\n",
    "\"she's\": \"she has / she is\",\n",
    "\"should've\": \"should have\",\n",
    "\"shouldn't\": \"should not\",\n",
    "\"shouldn't've\": \"should not have\",\n",
    "\"so've\": \"so have\",\n",
    "\"so's\": \"so as / so is\",\n",
    "\"that'd\": \"that would / that had\",\n",
    "\"that'd've\": \"that would have\",\n",
    "\"that's\": \"that has / that is\",\n",
    "\"there'd\": \"there had / there would\",\n",
    "\"there'd've\": \"there would have\",\n",
    "\"there's\": \"there has / there is\",\n",
    "\"they'd\": \"they had / they would\",\n",
    "\"they'd've\": \"they would have\",\n",
    "\"they'll\": \"they shall / they will\",\n",
    "\"they'll've\": \"they shall have / they will have\",\n",
    "\"they're\": \"they are\",\n",
    "\"they've\": \"they have\",\n",
    "\"to've\": \"to have\",\n",
    "\"wasn't\": \"was not\",\n",
    "\"we'd\": \"we had / we would\",\n",
    "\"we'd've\": \"we would have\",\n",
    "\"we'll\": \"we will\",\n",
    "\"we'll've\": \"we will have\",\n",
    "\"we're\": \"we are\",\n",
    "\"we've\": \"we have\",\n",
    "\"weren't\": \"were not\",\n",
    "\"what'll\": \"what shall / what will\",\n",
    "\"what'll've\": \"what shall have / what will have\",\n",
    "\"what're\": \"what are\",\n",
    "\"what's\": \"what has / what is\",\n",
    "\"what've\": \"what have\",\n",
    "\"when's\": \"when has / when is\",\n",
    "\"when've\": \"when have\",\n",
    "\"where'd\": \"where did\",\n",
    "\"where's\": \"where has / where is\",\n",
    "\"where've\": \"where have\",\n",
    "\"who'll\": \"who shall / who will\",\n",
    "\"who'll've\": \"who shall have / who will have\",\n",
    "\"who's\": \"who has / who is\",\n",
    "\"who've\": \"who have\",\n",
    "\"why's\": \"why has / why is\",\n",
    "\"why've\": \"why have\",\n",
    "\"will've\": \"will have\",\n",
    "\"won't\": \"will not\",\n",
    "\"won't've\": \"will not have\",\n",
    "\"would've\": \"would have\",\n",
    "\"wouldn't\": \"would not\",\n",
    "\"wouldn't've\": \"would not have\",\n",
    "\"y'all\": \"you all\",\n",
    "\"y'all'd\": \"you all would\",\n",
    "\"y'all'd've\": \"you all would have\",\n",
    "\"y'all're\": \"you all are\",\n",
    "\"y'all've\": \"you all have\",\n",
    "\"you'd\": \"you had / you would\",\n",
    "\"you'd've\": \"you would have\",\n",
    "\"you'll\": \"you shall / you will\",\n",
    "\"you'll've\": \"you shall have / you will have\",\n",
    "\"you're\": \"you are\",\n",
    "\"you've\": \"you have\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_contractions(text):\n",
    "    text = text.replace(\"â€™\",\"'\")\n",
    "    words = text.split()\n",
    "    reformed = [contractions[word] if word in contractions else word for word in words]\n",
    "    text = \" \".join(reformed)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apply replace contractions function to cleaned_text column in df_tweets dataframe\n",
    "df_tweets['cleaned_text'] = df_tweets['cleaned_text'].apply(lambda x: replace_contractions(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for twitter text cleaning \n",
    "def tweet_cleaner(text):\n",
    "    # convert emojis to text\n",
    "    text = emoji.demojize(text)\n",
    "    text = text.replace(\":\",\" \")\n",
    "        # remove punctuations, numbers, and special characters \n",
    "        #text = re.sub(\"[^a-zA-Z]\",\" \",text) \n",
    "        # remove whitespaces \n",
    "    text = ' '.join(text.split()) \n",
    "        # convert text to lowercase\n",
    "    text = text.lower()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets['cleaned_text'] = df_tweets['cleaned_text'].apply(lambda x: tweet_cleaner(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove rows containing missing values under the cleaned_text column \n",
    "df_tweets = df_tweets[df_tweets['cleaned_text'].notnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23000, 7)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tweets.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets.to_csv (r'E:\\twitter_SA\\tweets\\combined_tweets.csv', index = False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SN</th>\n",
       "      <th>Text</th>\n",
       "      <th>Username</th>\n",
       "      <th>Location</th>\n",
       "      <th>Date</th>\n",
       "      <th>Topic</th>\n",
       "      <th>cleaned_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>I am having a hard time with #Lent this year ...</td>\n",
       "      <td>wagagaikoko</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2021-03-10 19:10:00</td>\n",
       "      <td>corona</td>\n",
       "      <td>i am having a hard time with #lent this year d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>#Ø§Ù„Ø·Ø±ÙŠÙ‚_Ù…Ù‚Ø·ÙˆØ¹Citizens in Lebanon, are facing ...</td>\n",
       "      <td>RZgheib7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2021-03-10 19:10:00</td>\n",
       "      <td>corona</td>\n",
       "      <td>#Ø§Ù„Ø·Ø±ÙŠÙ‚_Ù…Ù‚Ø·ÙˆØ¹citizens in lebanon, are facing d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>I had the Corona already last January. So who...</td>\n",
       "      <td>cestlaviv</td>\n",
       "      <td>Carinthia - Austria</td>\n",
       "      <td>2021-03-10 19:10:00</td>\n",
       "      <td>corona</td>\n",
       "      <td>i had the corona already last january. so who ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Ooohh karma, you wonderful thing,man's restric...</td>\n",
       "      <td>FlavionsKimani</td>\n",
       "      <td>Nairobi Kenya</td>\n",
       "      <td>2021-03-10 19:10:00</td>\n",
       "      <td>corona</td>\n",
       "      <td>ooohh karma, you wonderful thing,man's restric...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>#Ø§Ù„Ø·Ø±ÙŠÙ‚_Ù…Ù‚Ø·ÙˆØ¹Citizens in Lebanon, are facing ...</td>\n",
       "      <td>Zizo90693787</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2021-03-10 19:10:00</td>\n",
       "      <td>corona</td>\n",
       "      <td>#Ø§Ù„Ø·Ø±ÙŠÙ‚_Ù…Ù‚Ø·ÙˆØ¹citizens in lebanon, are facing d...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   SN                                               Text        Username  \\\n",
       "0   0   I am having a hard time with #Lent this year ...     wagagaikoko   \n",
       "1   1   #Ø§Ù„Ø·Ø±ÙŠÙ‚_Ù…Ù‚Ø·ÙˆØ¹Citizens in Lebanon, are facing ...        RZgheib7   \n",
       "2   2   I had the Corona already last January. So who...       cestlaviv   \n",
       "3   3  Ooohh karma, you wonderful thing,man's restric...  FlavionsKimani   \n",
       "4   4   #Ø§Ù„Ø·Ø±ÙŠÙ‚_Ù…Ù‚Ø·ÙˆØ¹Citizens in Lebanon, are facing ...    Zizo90693787   \n",
       "\n",
       "               Location                Date   Topic  \\\n",
       "0                   NaN 2021-03-10 19:10:00  corona   \n",
       "1                   NaN 2021-03-10 19:10:00  corona   \n",
       "2  Carinthia - Austria  2021-03-10 19:10:00  corona   \n",
       "3        Nairobi Kenya  2021-03-10 19:10:00  corona   \n",
       "4                   NaN 2021-03-10 19:10:00  corona   \n",
       "\n",
       "                                        cleaned_text  \n",
       "0  i am having a hard time with #lent this year d...  \n",
       "1  #Ø§Ù„Ø·Ø±ÙŠÙ‚_Ù…Ù‚Ø·ÙˆØ¹citizens in lebanon, are facing d...  \n",
       "2  i had the corona already last january. so who ...  \n",
       "3  ooohh karma, you wonderful thing,man's restric...  \n",
       "4  #Ø§Ù„Ø·Ø±ÙŠÙ‚_Ù…Ù‚Ø·ÙˆØ¹citizens in lebanon, are facing d...  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#searching for tweets with 'giveaways'\n",
    "new_df = df_tweets[~df_tweets['cleaned_text'].str.contains('giveaway', na=False)]\n",
    "new_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22964, 7)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9302"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df.duplicated(subset=['Text']).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df=new_df.drop_duplicates(subset=['Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13662, 7)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.to_csv (r'E:\\twitter_SA\\tweets\\duplicate_removed.csv', index = False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SN</th>\n",
       "      <th>Text</th>\n",
       "      <th>Username</th>\n",
       "      <th>Location</th>\n",
       "      <th>Date</th>\n",
       "      <th>Topic</th>\n",
       "      <th>cleaned_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>I am having a hard time with #Lent this year ...</td>\n",
       "      <td>wagagaikoko</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2021-03-10 19:10:00</td>\n",
       "      <td>corona</td>\n",
       "      <td>i am having a hard time with #lent this year d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>#Ø§Ù„Ø·Ø±ÙŠÙ‚_Ù…Ù‚Ø·ÙˆØ¹Citizens in Lebanon, are facing ...</td>\n",
       "      <td>RZgheib7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2021-03-10 19:10:00</td>\n",
       "      <td>corona</td>\n",
       "      <td>#Ø§Ù„Ø·Ø±ÙŠÙ‚_Ù…Ù‚Ø·ÙˆØ¹citizens in lebanon, are facing d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>I had the Corona already last January. So who...</td>\n",
       "      <td>cestlaviv</td>\n",
       "      <td>Carinthia - Austria</td>\n",
       "      <td>2021-03-10 19:10:00</td>\n",
       "      <td>corona</td>\n",
       "      <td>i had the corona already last january. so who ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Ooohh karma, you wonderful thing,man's restric...</td>\n",
       "      <td>FlavionsKimani</td>\n",
       "      <td>Nairobi Kenya</td>\n",
       "      <td>2021-03-10 19:10:00</td>\n",
       "      <td>corona</td>\n",
       "      <td>ooohh karma, you wonderful thing,man's restric...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>#Ø§Ù„Ø·Ø±ÙŠÙ‚_Ù…Ù‚Ø·ÙˆØ¹Citizens in Lebanon, are facing ...</td>\n",
       "      <td>Zizo90693787</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2021-03-10 19:10:00</td>\n",
       "      <td>corona</td>\n",
       "      <td>#Ø§Ù„Ø·Ø±ÙŠÙ‚_Ù…Ù‚Ø·ÙˆØ¹citizens in lebanon, are facing d...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   SN                                               Text        Username  \\\n",
       "0   0   I am having a hard time with #Lent this year ...     wagagaikoko   \n",
       "1   1   #Ø§Ù„Ø·Ø±ÙŠÙ‚_Ù…Ù‚Ø·ÙˆØ¹Citizens in Lebanon, are facing ...        RZgheib7   \n",
       "2   2   I had the Corona already last January. So who...       cestlaviv   \n",
       "3   3  Ooohh karma, you wonderful thing,man's restric...  FlavionsKimani   \n",
       "4   4   #Ø§Ù„Ø·Ø±ÙŠÙ‚_Ù…Ù‚Ø·ÙˆØ¹Citizens in Lebanon, are facing ...    Zizo90693787   \n",
       "\n",
       "               Location                Date   Topic  \\\n",
       "0                   NaN 2021-03-10 19:10:00  corona   \n",
       "1                   NaN 2021-03-10 19:10:00  corona   \n",
       "2  Carinthia - Austria  2021-03-10 19:10:00  corona   \n",
       "3        Nairobi Kenya  2021-03-10 19:10:00  corona   \n",
       "4                   NaN 2021-03-10 19:10:00  corona   \n",
       "\n",
       "                                        cleaned_text  \n",
       "0  i am having a hard time with #lent this year d...  \n",
       "1  #Ø§Ù„Ø·Ø±ÙŠÙ‚_Ù…Ù‚Ø·ÙˆØ¹citizens in lebanon, are facing d...  \n",
       "2  i had the corona already last january. so who ...  \n",
       "3  ooohh karma, you wonderful thing,man's restric...  \n",
       "4  #Ø§Ù„Ø·Ø±ÙŠÙ‚_Ù…Ù‚Ø·ÙˆØ¹citizens in lebanon, are facing d...  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SN</th>\n",
       "      <th>Text</th>\n",
       "      <th>Username</th>\n",
       "      <th>Location</th>\n",
       "      <th>Date</th>\n",
       "      <th>Topic</th>\n",
       "      <th>cleaned_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4995</th>\n",
       "      <td>4995</td>\n",
       "      <td>Some great random memories of life before loc...</td>\n",
       "      <td>MeadowsFestival</td>\n",
       "      <td>The Meadows, Edinburgh</td>\n",
       "      <td>2021-03-09 16:34:00</td>\n",
       "      <td>lockdown</td>\n",
       "      <td>some great random memories of life before lock...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4996</th>\n",
       "      <td>4996</td>\n",
       "      <td>The First Female LockDown Housemate to own a ...</td>\n",
       "      <td>tosho_king</td>\n",
       "      <td>Lagos, Nigeria</td>\n",
       "      <td>2021-03-09 16:34:00</td>\n",
       "      <td>lockdown</td>\n",
       "      <td>the first female lockdown housemate to own a h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4997</th>\n",
       "      <td>4997</td>\n",
       "      <td>Happy Lockdown Period Anniversary everyoneðŸ¥°ðŸ¥°</td>\n",
       "      <td>_boujiee16_</td>\n",
       "      <td>Jamaica</td>\n",
       "      <td>2021-03-09 16:34:00</td>\n",
       "      <td>lockdown</td>\n",
       "      <td>happy lockdown period anniversary everyone smi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4998</th>\n",
       "      <td>4998</td>\n",
       "      <td>Bindura â€“ Gweshe Livelihoods DistributionsThe ...</td>\n",
       "      <td>care_agape</td>\n",
       "      <td>Harare, Zimbabwe</td>\n",
       "      <td>2021-03-09 16:34:00</td>\n",
       "      <td>lockdown</td>\n",
       "      <td>bindura â€“ gweshe livelihoods distributionsthe ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4999</th>\n",
       "      <td>4999</td>\n",
       "      <td>2weeks till our ONE year lockdown anniversary ...</td>\n",
       "      <td>HouseKelly17</td>\n",
       "      <td>Birkenhead, England</td>\n",
       "      <td>2021-03-09 16:34:00</td>\n",
       "      <td>lockdown</td>\n",
       "      <td>2weeks till our one year lockdown anniversary ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        SN                                               Text  \\\n",
       "4995  4995   Some great random memories of life before loc...   \n",
       "4996  4996   The First Female LockDown Housemate to own a ...   \n",
       "4997  4997      Happy Lockdown Period Anniversary everyoneðŸ¥°ðŸ¥°    \n",
       "4998  4998  Bindura â€“ Gweshe Livelihoods DistributionsThe ...   \n",
       "4999  4999  2weeks till our ONE year lockdown anniversary ...   \n",
       "\n",
       "             Username                Location                Date     Topic  \\\n",
       "4995  MeadowsFestival  The Meadows, Edinburgh 2021-03-09 16:34:00  lockdown   \n",
       "4996       tosho_king          Lagos, Nigeria 2021-03-09 16:34:00  lockdown   \n",
       "4997      _boujiee16_                 Jamaica 2021-03-09 16:34:00  lockdown   \n",
       "4998       care_agape        Harare, Zimbabwe 2021-03-09 16:34:00  lockdown   \n",
       "4999     HouseKelly17     Birkenhead, England 2021-03-09 16:34:00  lockdown   \n",
       "\n",
       "                                           cleaned_text  \n",
       "4995  some great random memories of life before lock...  \n",
       "4996  the first female lockdown housemate to own a h...  \n",
       "4997  happy lockdown period anniversary everyone smi...  \n",
       "4998  bindura â€“ gweshe livelihoods distributionsthe ...  \n",
       "4999  2weeks till our one year lockdown anniversary ...  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tweets.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets.to_csv (r'E:\\twitter_SA\\twitter_dataset_from_3mar-6mar\\1stphase_tweets.csv', index = False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.to_csv (r'pandemic_duplicate_removed.csv', index = False, header=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
